{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6b05a5",
   "metadata": {},
   "source": [
    "## Getting GTFS paths from files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8682ea3",
   "metadata": {},
   "source": [
    "This file is already generated, so no need to waste 20 minutes of your life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f8f46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ------------------------------\n",
    "# Define column types\n",
    "# ------------------------------\n",
    "stops_dtypes = {\n",
    "    'stop_name': 'object',\n",
    "    'parent_station': 'object',\n",
    "    'stop_id': 'object',\n",
    "    'stop_lat': 'float64',\n",
    "    'stop_lon': 'float64',\n",
    "    'location_type': 'float64',\n",
    "    'platform_code': 'object'\n",
    "}\n",
    "\n",
    "routes_dtypes = {\n",
    "    'route_long_name': 'object',\n",
    "    'route_short_name': 'object',\n",
    "    'agency_id': 'object',\n",
    "    'route_type': 'float64',\n",
    "    'route_id': 'object',\n",
    "    'route_color': 'object',\n",
    "    'route_text_color': 'object'\n",
    "}\n",
    "\n",
    "trips_dtypes = {\n",
    "    'route_id': 'object',\n",
    "    'service_id': 'object',\n",
    "    'trip_id': 'object'\n",
    "}\n",
    "\n",
    "stop_times_dtypes = {\n",
    "    'trip_id': 'object',\n",
    "    'arrival_time': 'object',\n",
    "    'departure_time': 'object',\n",
    "    'stop_id': 'object',\n",
    "    'stop_sequence': 'int64',\n",
    "    'pickup_type': 'float64',\n",
    "    'drop_off_type': 'float64'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Load CSVs with Dask\n",
    "# ------------------------------\n",
    "stops = dd.read_csv(\"stops.txt\", dtype=stops_dtypes)\n",
    "routes = dd.read_csv(\"routes.txt\", dtype=routes_dtypes)\n",
    "trips = dd.read_csv(\"trips.txt\", dtype=trips_dtypes)\n",
    "stop_times = dd.read_csv(\"stop_times.txt\", dtype=stop_times_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neilinn\\AppData\\Local\\Temp\\ipykernel_28428\\1989338775.py:9: UserWarning: `meta` is not specified, inferred from partial data.\n",
      "Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "\n",
      "  route_stops = ( stop_times_with_route.sort_values(['route_id', 'trip_id', 'stop_sequence']).groupby(['route_short_name','route_id','trip_id'])['stop_name'].apply(list).reset_index() )\n"
     ]
    }
   ],
   "source": [
    "# Merge stop_times with trips to get route_id for each stop\n",
    "stop_times_with_route = stop_times.merge(trips[['trip_id', 'route_id']], on='trip_id', how='left')\n",
    "\n",
    "# Merge with stops to get stop_name\n",
    "stop_times_with_route = stop_times_with_route.merge(stops[['stop_id', 'stop_name']], on='stop_id', how='left')\n",
    "stop_times_with_route = stop_times_with_route.merge(routes[['route_id', 'route_short_name']], on='route_id', how='left')\n",
    "\n",
    "# Group by route_id and aggregate stop_name into a list (ordered by stop_sequence)\n",
    "route_stops = ( stop_times_with_route.sort_values(['route_id', 'trip_id', 'stop_sequence']).groupby(['route_short_name','route_id','trip_id'])['stop_name'].apply(list).reset_index() )\n",
    "route_stops = route_stops.compute()\n",
    "\n",
    "# Optional: remove duplicates within each route\n",
    "route_stops['stop_name'] = route_stops['stop_name'].apply(lambda x: list(dict.fromkeys(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20107bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_station_name(station: str) -> str:\n",
    "    \"\"\"Clean one station name: remove hyphens, brackets, and trim spaces.\"\"\"\n",
    "    if pd.isna(station):\n",
    "        return \"\"\n",
    "    station = station.replace('-', ' ')\n",
    "    station = station.replace('(', ' ')\n",
    "    station = station.replace(')', ' ')\n",
    "    station = station.replace('Hauptbahnhof', 'Hbf')\n",
    "    station = re.sub(r'\\s+', ' ', station)\n",
    "    return station.strip()\n",
    "\n",
    "# If stop_name column is a list of stations\n",
    "#route_stops['stop_name_fixed'] = route_stops['stop_name'].apply(lambda stops: '|'.join(clean_station_name(s) for s in stops))\n",
    "route_stops['stop_name_fixed'] = route_stops['stop_name'].apply(lambda stops: [clean_station_name(s) for s in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_file = route_stops[['stop_name_fixed', 'route_short_name']]\n",
    "gtfs_file['stop_name_fixed'] = gtfs_file['stop_name_fixed'].apply(lambda stops: '|'.join(s for s in stops))\n",
    "gtfs_file.to_csv(\"gtfs_paths.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a865da",
   "metadata": {},
   "source": [
    "## Generating all possible paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a119e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('DBtrainrides.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd16a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_station_name(station: str) -> str:\n",
    "    \"\"\"Clean one station name: remove hyphens, brackets, and trim spaces.\"\"\"\n",
    "    station = station.replace('-', ' ')       # replace hyphens with spaces\n",
    "    station = station.replace('(', ' ')    # replace brackets with spaces\n",
    "    station = station.replace(')', ' ')    # replace brackets with spaces\n",
    "    station = station.replace('Hauptbahnhof', 'Hbf')\n",
    "    station = re.sub(r'\\s+', ' ', station)\n",
    "    return station.strip()\n",
    "\n",
    "def clean_path(path: str) -> str:\n",
    "    \"\"\"Clean a full path, applying cleaning per station.\"\"\"\n",
    "    if not isinstance(path, str) or not path.strip():\n",
    "        return ''\n",
    "    stations = path.split('|')\n",
    "    cleaned = [clean_station_name(s) for s in stations]\n",
    "    return '|'.join(cleaned)\n",
    "\n",
    "# Apply cleaning\n",
    "data['path'] = data['path'].apply(clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['station'] = data['station'].apply(clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_full_routes = []\n",
    "\n",
    "##each station in path do the clean, right now whole path is being cleaned\n",
    "line_numbers = data.loc[data['line'].astype(str).str.match(r'^\\d+[a-zA-Z]?$'), 'line'].unique().tolist()\n",
    "\n",
    "for number in line_numbers:\n",
    "    subset = data[data['line'].astype(str) == str(number)]\n",
    "\n",
    "    #subset = data[data['line'].astype(str).match(r'^\\d+[a-zA-Z]?$')]\n",
    "    # Filter paths starting with Rostock Hbf\n",
    "    #subset = subset[subset['path'].astype(str).str.strip().str.lower().str.startswith('rostock hbf')]\n",
    "\n",
    "    # Create full path by appending the next station\n",
    "    subset['full_path'] = subset['path'].fillna('').astype(str).str.strip()\n",
    "    subset['full_path'] = subset.apply(\n",
    "        lambda row: row['full_path'] + '|' + row['station'] if row['full_path'] else row['station'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop duplicates to speed up the next step\n",
    "    subset = subset.drop_duplicates(subset=['full_path']).reset_index(drop=True)\n",
    "\n",
    "    # Sort the rows by path length to potentially make it faster\n",
    "    subset['path_length'] = subset['full_path'].str.count(r'\\|') + 1\n",
    "    subset = subset.sort_values('path_length').reset_index(drop=True)\n",
    "\n",
    "    # Identify full routes: a path is full if no other path starts with it\n",
    "    all_paths = subset['full_path'].tolist()\n",
    "    full_routes = [p for i, p in enumerate(all_paths) \n",
    "                if not any(other != p and other.startswith(p + '|') for other in all_paths)]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(full_routes, columns=['full_path'])\n",
    "    df['line'] = number\n",
    "    all_full_routes.append(df)\n",
    "\n",
    "full_routes_df = pd.concat(all_full_routes, ignore_index=True)\n",
    "# Save the full_routes_df to a CSV file\n",
    "# Add an empty column called 'fixed_line' to the DataFrame\n",
    "full_routes_df['fixed_line'] = ''\n",
    "full_routes_df.to_csv('full_routes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d91b3",
   "metadata": {},
   "source": [
    "## Mapping lines with corresponding path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f202112",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_routes = pd.read_csv(\"full_routes.csv\")\n",
    "\n",
    "# Clean and split 'full_path' into a list at '|'\n",
    "full_routes['full_path'] = (\n",
    "    full_routes['full_path']\n",
    "    .str.replace('Hauptbahnhof', 'Hbf')\n",
    "    .str.strip()\n",
    "    .apply(lambda x: [s.strip() for s in x.split('|') if s.strip()])\n",
    ")\n",
    "\n",
    "route_lookup = {}\n",
    "\n",
    "for _, row in route_stops.iterrows():\n",
    "    stops = tuple(row['stop_name_fixed'])\n",
    "    reversed_stops = tuple(row['stop_name_fixed'][::-1])\n",
    "    route_name = row['route_short_name']\n",
    "\n",
    "    # Add both directions to the lookup\n",
    "    route_lookup[stops] = route_name\n",
    "    route_lookup[reversed_stops] = route_name\n",
    "\n",
    "# Match each full_path_clean to its route_short_name if found\n",
    "def find_route_name(path_list):\n",
    "    \"\"\"Return route_short_name if list (or its reverse) matches any route.\"\"\"\n",
    "    return route_lookup.get(tuple(path_list), None)\n",
    "\n",
    "full_routes['fixed_line'] = full_routes['full_path'].apply(find_route_name)\n",
    "full_routes['full_path'] = full_routes['full_path'].apply(lambda stops: '|'.join(s for s in stops))\n",
    "full_routes.to_csv(\"full_routes_mapped1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "809a2ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6347\n"
     ]
    }
   ],
   "source": [
    "empty_count = (full_routes['fixed_line'].isna() | (full_routes['fixed_line'] == '')).sum()\n",
    "print(empty_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6df5b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "file_paths = pd.read_csv(\"gtfs_paths.csv\")\n",
    "\n",
    "filtered_rows = file_paths[\n",
    "    file_paths['stop_name_fixed'].str.contains('Stuttgart Schwabstraße', na=False) & \n",
    "    file_paths['route_short_name'].str.contains('1', na=False)\n",
    "]['route_short_name'].unique()\n",
    "\n",
    "print(filtered_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a9a451c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['439' 'S4']\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "file_paths = pd.read_csv(\"gtfs_paths.csv\")\n",
    "\n",
    "filtered_rows = file_paths[\n",
    "    file_paths['stop_name_fixed'].str.contains('München Rosenheimer Platz|München Ost|München Leuchtenbergring', na=False, regex=False) & \n",
    "    file_paths['route_short_name'].str.contains('4', na=False)\n",
    "]['route_short_name'].unique()\n",
    "\n",
    "print(filtered_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "28e5fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "gtfs_paths = pd.read_csv(\"gtfs_paths.csv\")\n",
    "line_mappings = pd.read_csv(\"correct_line_mapping.csv\")\n",
    "extra = pd.read_csv(\"correct_line_mapping.csv\")\n",
    "pattern = re.compile(r'^\\d+[a-zA-Z]?$')\n",
    "\n",
    "#line_mappings = line_mappings[line_mappings['line'].isin(['1', '20', '18'])]\n",
    "#extra = extra[extra['line'].isin(['1', '20', '18'])]\n",
    "empty_count = (line_mappings['fixed_line'].isna() | (line_mappings['fixed_line'] == '')).sum()\n",
    "print(empty_count)\n",
    "\n",
    "for i, row in line_mappings.iterrows():\n",
    "    if pd.notna(row['fixed_line']) and row['fixed_line'] != '':\n",
    "        continue\n",
    "\n",
    "    filtered_rows = extra[\n",
    "        extra['full_path'].str.contains(row['full_path'], na=False) & \n",
    "        extra['fixed_line'].str.match(rf'^[A-Za-z]*{row['line']}$')\n",
    "    ]['fixed_line'].unique()\n",
    "\n",
    "    filtered = [x for x in filtered_rows if not pattern.match(x)]\n",
    "\n",
    "    if len(filtered) == 1:\n",
    "        line_mappings.loc[i,'fixed_line'] = filtered[0]\n",
    "\n",
    "line_mappings.to_csv(\"full_routes_mapped1.csv\", index=False)\n",
    "\n",
    "empty_count = (line_mappings['fixed_line'].isna() | (line_mappings['fixed_line'] == '')).sum()\n",
    "print(empty_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bceecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "gtfs_paths = pd.read_csv(\"gtfs_paths.csv\")\n",
    "line_mappings = pd.read_csv(\"correct_line_mapping.csv\")\n",
    "extra = pd.read_csv(\"correct_line_mapping.csv\")\n",
    "pattern = re.compile(r'^\\d+[a-zA-Z]?$')\n",
    "\n",
    "#line_mappings = line_mappings[line_mappings['line'].isin(['1', '20', '18'])]\n",
    "#extra = extra[extra['line'].isin(['1', '20', '18'])]\n",
    "empty_count = (line_mappings['fixed_line'].isna() | (line_mappings['fixed_line'] == '')).sum()\n",
    "print(empty_count)\n",
    "\n",
    "for i, row in line_mappings.iterrows():\n",
    "    if pd.notna(row['fixed_line']) and row['fixed_line'] != '':\n",
    "        continue\n",
    "\n",
    "    filtered_rows = gtfs_paths[\n",
    "        gtfs_paths['stop_name_fixed'].str.contains(row['full_path'], na=False) & \n",
    "        gtfs_paths['route_short_name'].str.match(rf'^[A-Za-z]*{row['line']}$')\n",
    "    ]['route_short_name'].unique()\n",
    "\n",
    "    filtered = [x for x in filtered_rows if not pattern.match(x)]\n",
    "\n",
    "    if len(filtered) == 1:\n",
    "        line_mappings.loc[i,'fixed_line'] = filtered[0]\n",
    "\n",
    "line_mappings.to_csv(\"full_routes_mapped1.csv\", index=False)\n",
    "\n",
    "empty_count = (line_mappings['fixed_line'].isna() | (line_mappings['fixed_line'] == '')).sum()\n",
    "print(empty_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a0270ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: is_empty, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "line_mappings = pd.read_csv(\"correct_line_mapping.csv\")\n",
    "line_mappings['is_empty'] = line_mappings['fixed_line'].isna() | (line_mappings['fixed_line'] == '')\n",
    "empty_counts_per_line = line_mappings.groupby('line')['is_empty'].sum()\n",
    "empty_counts_per_line = empty_counts_per_line[empty_counts_per_line > 0]\n",
    "print(empty_counts_per_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "de41e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "empty_count = (line_mappings['fixed_line'].isna() | (line_mappings['fixed_line'] == '')).sum()\n",
    "print(empty_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-db-delays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
